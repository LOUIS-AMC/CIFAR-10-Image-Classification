{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d485e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff2cc3d-4024-4502-86bc-ac39c06fc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes') # load the file into a dictionary\n",
    "    return dict\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0737a-78ab-409b-a6c7-aff65a1f8712",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KNN Classification Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b05d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Results in data_batch_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.30      0.38       326\n",
      "           1       0.15      0.57      0.23        47\n",
      "           2       0.40      0.21      0.27       433\n",
      "           3       0.17      0.29      0.21       120\n",
      "           4       0.42      0.18      0.25       419\n",
      "           5       0.18      0.33      0.23        88\n",
      "           6       0.20      0.26      0.22       173\n",
      "           7       0.13      0.64      0.21        44\n",
      "           8       0.56      0.37      0.44       313\n",
      "           9       0.10      0.51      0.16        37\n",
      "\n",
      "    accuracy                           0.28      2000\n",
      "   macro avg       0.28      0.37      0.26      2000\n",
      "weighted avg       0.39      0.28      0.30      2000\n",
      "\n",
      "KNN Results in data_batch_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.34      0.42       327\n",
      "           1       0.12      0.47      0.19        51\n",
      "           2       0.35      0.17      0.23       377\n",
      "           3       0.14      0.23      0.17       133\n",
      "           4       0.47      0.18      0.26       504\n",
      "           5       0.13      0.39      0.19        67\n",
      "           6       0.20      0.27      0.23       151\n",
      "           7       0.16      0.51      0.24        63\n",
      "           8       0.60      0.41      0.49       302\n",
      "           9       0.08      0.56      0.13        25\n",
      "\n",
      "    accuracy                           0.28      2000\n",
      "   macro avg       0.28      0.35      0.26      2000\n",
      "weighted avg       0.40      0.28      0.30      2000\n",
      "\n",
      "KNN Results in data_batch_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.32      0.40       321\n",
      "           1       0.16      0.52      0.24        56\n",
      "           2       0.44      0.16      0.24       468\n",
      "           3       0.15      0.22      0.18       126\n",
      "           4       0.45      0.21      0.29       421\n",
      "           5       0.14      0.41      0.20        71\n",
      "           6       0.22      0.31      0.26       144\n",
      "           7       0.14      0.52      0.22        54\n",
      "           8       0.56      0.39      0.46       303\n",
      "           9       0.10      0.64      0.17        36\n",
      "\n",
      "    accuracy                           0.28      2000\n",
      "   macro avg       0.29      0.37      0.27      2000\n",
      "weighted avg       0.41      0.28      0.30      2000\n",
      "\n",
      "KNN Results in data_batch_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.26      0.33       330\n",
      "           1       0.11      0.53      0.18        43\n",
      "           2       0.41      0.21      0.27       394\n",
      "           3       0.25      0.34      0.28       125\n",
      "           4       0.48      0.23      0.31       447\n",
      "           5       0.22      0.34      0.27       136\n",
      "           6       0.16      0.26      0.20       124\n",
      "           7       0.12      0.57      0.20        44\n",
      "           8       0.60      0.37      0.46       325\n",
      "           9       0.08      0.47      0.13        32\n",
      "\n",
      "    accuracy                           0.29      2000\n",
      "   macro avg       0.29      0.36      0.26      2000\n",
      "weighted avg       0.41      0.29      0.31      2000\n",
      "\n",
      "KNN Results in data_batch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.30      0.35       282\n",
      "           1       0.14      0.70      0.24        44\n",
      "           2       0.37      0.20      0.26       405\n",
      "           3       0.16      0.22      0.18       153\n",
      "           4       0.48      0.19      0.28       451\n",
      "           5       0.14      0.30      0.19        96\n",
      "           6       0.20      0.29      0.24       130\n",
      "           7       0.10      0.47      0.16        40\n",
      "           8       0.63      0.33      0.43       362\n",
      "           9       0.09      0.49      0.15        37\n",
      "\n",
      "    accuracy                           0.27      2000\n",
      "   macro avg       0.27      0.35      0.25      2000\n",
      "weighted avg       0.40      0.27      0.29      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "\n",
    "# initialize the accuracy lists\n",
    "tr_acclst = []\n",
    "val_acclst = []\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "for i in data:\n",
    "    d = unpickle(f'data_batch_{i}')\n",
    "    X = d[b'data'] # assign into X\n",
    "    y = d[b'labels'] # assign into Y\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(d[b'data'], d[b'labels'], \n",
    "                                test_size=0.2, random_state=seed, shuffle=True)\n",
    "    # Transform the data\n",
    "    sclr = StandardScaler()\n",
    "    X_tr = sclr.fit_transform(X_tr)\n",
    "    X_val = sclr.fit_transform(X_val)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', weights='uniform',\n",
    "                               leaf_size=30, n_jobs=-1)\n",
    "    knn.fit(X_tr, y_tr)\n",
    "    \n",
    "    knn_tr_pred = knn.predict(X_tr)\n",
    "    knn_val_pred = knn.predict(X_val)\n",
    "    \n",
    "    # Gets the accuracy score\n",
    "    tr_acc = accuracy_score(knn_tr_pred, y_tr)\n",
    "    val_acc = accuracy_score(knn_val_pred, y_val)\n",
    "    \n",
    "    tr_acclst.append(tr_acc)\n",
    "    val_acclst.append(val_acc)\n",
    "    \n",
    "    print(f'KNN Results in data_batch_{i}')\n",
    "    print(classification_report(knn_val_pred, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ea0ce-3d21-412a-8176-150334339f3c",
   "metadata": {},
   "source": [
    "## Logistic Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3497ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classifier Results in data_batch_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.44      0.48       223\n",
      "           1       0.51      0.42      0.46       225\n",
      "           2       0.24      0.30      0.27       181\n",
      "           3       0.26      0.31      0.28       175\n",
      "           4       0.32      0.30      0.31       186\n",
      "           5       0.28      0.25      0.27       182\n",
      "           6       0.43      0.42      0.43       234\n",
      "           7       0.43      0.51      0.47       186\n",
      "           8       0.47      0.45      0.46       215\n",
      "           9       0.42      0.44      0.43       193\n",
      "\n",
      "    accuracy                           0.39      2000\n",
      "   macro avg       0.39      0.38      0.39      2000\n",
      "weighted avg       0.40      0.39      0.39      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classifier Results in data_batch_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.44      0.43       203\n",
      "           1       0.50      0.42      0.46       233\n",
      "           2       0.29      0.28      0.28       192\n",
      "           3       0.25      0.32      0.28       170\n",
      "           4       0.27      0.32      0.29       162\n",
      "           5       0.31      0.35      0.33       182\n",
      "           6       0.46      0.41      0.43       228\n",
      "           7       0.42      0.42      0.42       204\n",
      "           8       0.54      0.50      0.52       227\n",
      "           9       0.39      0.37      0.38       199\n",
      "\n",
      "    accuracy                           0.39      2000\n",
      "   macro avg       0.39      0.38      0.38      2000\n",
      "weighted avg       0.40      0.39      0.39      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classifier Results in data_batch_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.43      0.43       203\n",
      "           1       0.56      0.44      0.49       239\n",
      "           2       0.26      0.29      0.28       156\n",
      "           3       0.27      0.26      0.27       199\n",
      "           4       0.35      0.40      0.37       172\n",
      "           5       0.32      0.33      0.33       209\n",
      "           6       0.42      0.38      0.40       224\n",
      "           7       0.40      0.43      0.41       185\n",
      "           8       0.52      0.51      0.52       216\n",
      "           9       0.46      0.53      0.49       197\n",
      "\n",
      "    accuracy                           0.40      2000\n",
      "   macro avg       0.40      0.40      0.40      2000\n",
      "weighted avg       0.41      0.40      0.40      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classifier Results in data_batch_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.37      0.39       210\n",
      "           1       0.47      0.51      0.49       200\n",
      "           2       0.29      0.30      0.30       188\n",
      "           3       0.37      0.30      0.33       210\n",
      "           4       0.25      0.33      0.29       162\n",
      "           5       0.37      0.36      0.36       215\n",
      "           6       0.51      0.43      0.47       239\n",
      "           7       0.34      0.45      0.39       158\n",
      "           8       0.51      0.45      0.48       229\n",
      "           9       0.42      0.44      0.43       189\n",
      "\n",
      "    accuracy                           0.40      2000\n",
      "   macro avg       0.40      0.39      0.39      2000\n",
      "weighted avg       0.40      0.40      0.40      2000\n",
      "\n",
      "Logistic Classifier Results in data_batch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.45      0.46       211\n",
      "           1       0.45      0.44      0.45       221\n",
      "           2       0.21      0.32      0.26       141\n",
      "           3       0.31      0.35      0.33       189\n",
      "           4       0.31      0.34      0.32       162\n",
      "           5       0.35      0.36      0.35       202\n",
      "           6       0.45      0.37      0.41       228\n",
      "           7       0.38      0.37      0.38       195\n",
      "           8       0.59      0.48      0.53       231\n",
      "           9       0.49      0.46      0.48       220\n",
      "\n",
      "    accuracy                           0.40      2000\n",
      "   macro avg       0.40      0.39      0.40      2000\n",
      "weighted avg       0.42      0.40      0.41      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "\n",
    "# initialize the accuracy lists\n",
    "tr_acclst = []\n",
    "val_acclst = []\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "for i in data:\n",
    "    d = unpickle(f'data_batch_{i}')\n",
    "    X = d[b'data'] # assign into X\n",
    "    y = d[b'labels'] # assign into Y\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(d[b'data'], d[b'labels'], \n",
    "                                test_size=0.2, random_state=seed, shuffle=True)\n",
    "    # Transform the data\n",
    "    sclr = StandardScaler()\n",
    "    X_tr = sclr.fit_transform(X_tr)\n",
    "    X_val = sclr.fit_transform(X_val)\n",
    "    \n",
    "    log = LogisticRegression(penalty='l2', multi_class='auto', solver='lbfgs', \n",
    "                             C=0.001, max_iter=100, random_state=seed)\n",
    "\n",
    "    log.fit(X_tr, y_tr)\n",
    "\n",
    "    log_tr_pred = log.predict(X_tr)\n",
    "    log_val_pred = log.predict(X_val)\n",
    "    \n",
    "    # Gets the accuracy score\n",
    "    tr_acc = accuracy_score(log_tr_pred, y_tr)\n",
    "    val_acc = accuracy_score(log_val_pred, y_val)\n",
    "    \n",
    "    tr_acclst.append(tr_acc)\n",
    "    val_acclst.append(val_acc)\n",
    "\n",
    "\n",
    "    print(f'Logistic Classifier Results in data_batch_{i}')\n",
    "    print(classification_report(log_val_pred, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ed7fa-84a5-41e3-be93-d3e338e1f011",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba66f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Results in data_batch_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.48      0.49       197\n",
      "           1       0.51      0.56      0.53       169\n",
      "           2       0.32      0.36      0.34       202\n",
      "           3       0.29      0.26      0.27       225\n",
      "           4       0.43      0.34      0.38       222\n",
      "           5       0.33      0.27      0.30       197\n",
      "           6       0.43      0.47      0.45       207\n",
      "           7       0.45      0.55      0.50       183\n",
      "           8       0.58      0.53      0.56       225\n",
      "           9       0.41      0.47      0.44       173\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.42      0.43      0.42      2000\n",
      "weighted avg       0.42      0.42      0.42      2000\n",
      "\n",
      "MLP Classifier Results in data_batch_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52       209\n",
      "           1       0.48      0.52      0.50       183\n",
      "           2       0.32      0.27      0.29       215\n",
      "           3       0.32      0.32      0.32       217\n",
      "           4       0.30      0.29      0.30       194\n",
      "           5       0.33      0.31      0.32       214\n",
      "           6       0.43      0.45      0.44       196\n",
      "           7       0.45      0.49      0.47       185\n",
      "           8       0.63      0.59      0.61       223\n",
      "           9       0.40      0.45      0.42       164\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.42      0.42      0.42      2000\n",
      "weighted avg       0.42      0.42      0.42      2000\n",
      "\n",
      "MLP Classifier Results in data_batch_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.48      0.50       209\n",
      "           1       0.50      0.48      0.49       193\n",
      "           2       0.34      0.27      0.30       225\n",
      "           3       0.24      0.23      0.23       197\n",
      "           4       0.35      0.35      0.35       199\n",
      "           5       0.33      0.31      0.32       226\n",
      "           6       0.43      0.53      0.47       166\n",
      "           7       0.42      0.45      0.44       186\n",
      "           8       0.58      0.63      0.60       196\n",
      "           9       0.46      0.52      0.49       203\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.42      0.42      0.42      2000\n",
      "weighted avg       0.41      0.42      0.42      2000\n",
      "\n",
      "MLP Classifier Results in data_batch_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.45      0.45       182\n",
      "           1       0.40      0.57      0.47       151\n",
      "           2       0.26      0.28      0.27       189\n",
      "           3       0.31      0.23      0.26       228\n",
      "           4       0.34      0.34      0.34       216\n",
      "           5       0.40      0.37      0.39       230\n",
      "           6       0.39      0.42      0.40       185\n",
      "           7       0.47      0.50      0.48       193\n",
      "           8       0.61      0.53      0.57       232\n",
      "           9       0.44      0.45      0.45       194\n",
      "\n",
      "    accuracy                           0.41      2000\n",
      "   macro avg       0.41      0.41      0.41      2000\n",
      "weighted avg       0.41      0.41      0.41      2000\n",
      "\n",
      "MLP Classifier Results in data_batch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.44      0.45       203\n",
      "           1       0.44      0.55      0.49       176\n",
      "           2       0.33      0.33      0.33       211\n",
      "           3       0.31      0.27      0.29       239\n",
      "           4       0.34      0.34      0.34       185\n",
      "           5       0.31      0.32      0.32       204\n",
      "           6       0.37      0.38      0.37       184\n",
      "           7       0.44      0.51      0.47       167\n",
      "           8       0.67      0.55      0.60       229\n",
      "           9       0.42      0.43      0.43       202\n",
      "\n",
      "    accuracy                           0.41      2000\n",
      "   macro avg       0.41      0.41      0.41      2000\n",
      "weighted avg       0.41      0.41      0.41      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "\n",
    "# initialize the accuracy lists\n",
    "tr_acclst = []\n",
    "val_acclst = []\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "for i in data:\n",
    "    d = unpickle(f'data_batch_{i}')\n",
    "    X = d[b'data'] # assign into X\n",
    "    y = d[b'labels'] # assign into Y\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(d[b'data'], d[b'labels'], \n",
    "                                test_size=0.2, random_state=seed, shuffle=True)\n",
    "    # Transform the data\n",
    "    sclr = StandardScaler()\n",
    "    X_tr = sclr.fit_transform(X_tr)\n",
    "    X_val = sclr.fit_transform(X_val)\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), solver='sgd',\n",
    "                    alpha=0.001, batch_size=32, activation='relu', \n",
    "                    max_iter=100, learning_rate='constant', \n",
    "                    learning_rate_init=0.001,\n",
    "                    random_state=seed)\n",
    "\n",
    "\n",
    "    mlp.fit(X_tr, y_tr)\n",
    "    mlp_tr_pred = mlp.predict(X_tr)\n",
    "    mlp_val_pred = mlp.predict(X_val)\n",
    "    \n",
    "    # Gets the accuracy score\n",
    "    tr_acc = accuracy_score(mlp_tr_pred, y_tr)\n",
    "    val_acc = accuracy_score(mlp_val_pred, y_val)\n",
    "\n",
    "    tr_acclst.append(tr_acc)\n",
    "    val_acclst.append(val_acc)\n",
    "\n",
    "    print(f'MLP Classifier Results in data_batch_{i}')\n",
    "    print(classification_report(mlp_val_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d046391-0836-4a8e-8124-b8d88d087e08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a91e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results in data_batch_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.51      0.52       199\n",
      "           1       0.44      0.50      0.47       165\n",
      "           2       0.24      0.44      0.31       122\n",
      "           3       0.21      0.33      0.26       134\n",
      "           4       0.42      0.31      0.36       239\n",
      "           5       0.43      0.35      0.38       199\n",
      "           6       0.53      0.40      0.45       304\n",
      "           7       0.40      0.55      0.46       160\n",
      "           8       0.60      0.51      0.55       245\n",
      "           9       0.55      0.47      0.51       233\n",
      "\n",
      "    accuracy                           0.43      2000\n",
      "   macro avg       0.44      0.44      0.43      2000\n",
      "weighted avg       0.46      0.43      0.44      2000\n",
      "\n",
      "Random Forest Results in data_batch_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56       193\n",
      "           1       0.50      0.44      0.47       222\n",
      "           2       0.27      0.33      0.30       150\n",
      "           3       0.26      0.41      0.32       136\n",
      "           4       0.36      0.32      0.34       216\n",
      "           5       0.35      0.40      0.38       179\n",
      "           6       0.52      0.40      0.45       262\n",
      "           7       0.45      0.47      0.46       191\n",
      "           8       0.59      0.58      0.58       211\n",
      "           9       0.48      0.37      0.41       240\n",
      "\n",
      "    accuracy                           0.43      2000\n",
      "   macro avg       0.43      0.43      0.43      2000\n",
      "weighted avg       0.44      0.43      0.43      2000\n",
      "\n",
      "Random Forest Results in data_batch_3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.51      0.52       206\n",
      "           1       0.53      0.42      0.47       234\n",
      "           2       0.27      0.30      0.28       162\n",
      "           3       0.22      0.34      0.26       120\n",
      "           4       0.42      0.37      0.40       226\n",
      "           5       0.37      0.39      0.38       198\n",
      "           6       0.52      0.46      0.49       233\n",
      "           7       0.39      0.46      0.43       168\n",
      "           8       0.55      0.58      0.57       199\n",
      "           9       0.55      0.49      0.52       254\n",
      "\n",
      "    accuracy                           0.44      2000\n",
      "   macro avg       0.44      0.43      0.43      2000\n",
      "weighted avg       0.45      0.44      0.44      2000\n",
      "\n",
      "Random Forest Results in data_batch_4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.47      0.45       171\n",
      "           1       0.41      0.52      0.46       170\n",
      "           2       0.29      0.31      0.30       182\n",
      "           3       0.25      0.30      0.28       142\n",
      "           4       0.35      0.38      0.36       193\n",
      "           5       0.43      0.42      0.43       216\n",
      "           6       0.48      0.41      0.44       232\n",
      "           7       0.42      0.50      0.46       175\n",
      "           8       0.64      0.50      0.56       258\n",
      "           9       0.52      0.40      0.45       261\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.42      0.42      0.42      2000\n",
      "weighted avg       0.44      0.42      0.43      2000\n",
      "\n",
      "Random Forest Results in data_batch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.49      0.49       202\n",
      "           1       0.43      0.49      0.46       192\n",
      "           2       0.17      0.40      0.24        90\n",
      "           3       0.25      0.36      0.30       151\n",
      "           4       0.43      0.34      0.38       228\n",
      "           5       0.39      0.33      0.36       246\n",
      "           6       0.49      0.37      0.42       248\n",
      "           7       0.36      0.48      0.41       146\n",
      "           8       0.60      0.51      0.55       221\n",
      "           9       0.58      0.44      0.50       276\n",
      "\n",
      "    accuracy                           0.42      2000\n",
      "   macro avg       0.42      0.42      0.41      2000\n",
      "weighted avg       0.45      0.42      0.43      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "\n",
    "# initialize the accuracy lists\n",
    "tr_acclst = []\n",
    "val_acclst = []\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "for i in data:\n",
    "    d = unpickle(f'data_batch_{i}')\n",
    "    X = d[b'data'] # assign into X\n",
    "    y = d[b'labels'] # assign into Y\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(d[b'data'], d[b'labels'], \n",
    "                                test_size=0.2, random_state=seed, shuffle=True)\n",
    "    # Transform the data\n",
    "    sclr = StandardScaler()\n",
    "    X_tr = sclr.fit_transform(X_tr)\n",
    "    X_val = sclr.fit_transform(X_val)\n",
    "\n",
    "    dt = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=20, min_samples_split=5,\n",
    "                                min_samples_leaf=2, n_jobs=-1, random_state=seed)\n",
    "    dt.fit(X_tr, y_tr)\n",
    "\n",
    "    dt_tr_pred = dt.predict(X_tr)\n",
    "    dt_val_pred = dt.predict(X_val)\n",
    "    \n",
    "    # Gets the accuracy score\n",
    "    tr_acc = accuracy_score(dt_tr_pred, y_tr)\n",
    "    val_acc = accuracy_score(dt_val_pred, y_val)\n",
    "    \n",
    "    tr_acclst.append(tr_acc)\n",
    "    val_acclst.append(val_acc)\n",
    "    \n",
    "    print(f'Random Forest Results in data_batch_{i}')\n",
    "    print(classification_report(dt_val_pred, y_val))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
